Building Pipeline:
1> Create a GitHub repo and clone it in local (Add experiments).
2> Add src folder along with all components(run them individually).
3> Add data, models, reports directories to .gitignore file
4> Now git add, commit, push
    Note:
    a> Incase we forget to add .gitignore while creating the repository, we can do that at a 
       later stage by adding a ".gitignore" file without any extension
    b> also to add any file/folder to .gitignore so that they are not tracked by git, use the below command format:
       git rm -r --cached 'reports\', which will untrack all files under 'reports'
       also, make sure to commit it like : git commit -m "stopped tracking reports"

Setting up dvc pipeline (without params)
5> Create dvc.yaml file and add stages to it.
   Crash course on yaml in this repo: https://github.com/Arko2016/Learning-YAML-crashcourse
6> dvc init then do "dvc repro" to test the pipeline automation.
    Note:
    a> Once dvc repro is execute successfully do 'git add dvc.lock' to add the changes for git tracking
    b> Check using command 'dvc dag' to see flow of execution
    c> The generated dvc lock file will generate a unique hash value for each update made for each stage
       This will help to revert back to a particular version through git, since we have added dvc.lock to tracking
7> Now git add, commit, push

Setting up dvc pipeline (with params)
8> add params.yaml file
9> Add the params setup in the files under src folder where the corresponding parameters in params.yaml experiments
   In our case, it will be in data_ingestion, feature engineering and model_building
10> Do "dvc repro" again to test the pipeline along with the params, followed by git add dvc.lock to continue git tracking
11> Now perform git add, commit, push

Expermients with DVC:
12> pip install dvclive (if not already installed)
13> dvclive is used for tracking experiments
    in our case, dvclive code needs to be added to model_evaluation file under src folder since that will be used to track the model performance
14> Experment tracking using -> "dvc exp run", it will create 
    a> a new dvc.yaml(if already not there)
    b> dvclive directory (each run will be considered as an experiment by DVC)
15> Do "dvc exp show" on terminal to see the experiments or use extension on VSCode (install dvc extension)
16> Do "dvc exp remove {exp-name}" to remove exp (optional) 
    Additionally, do "dvc exp apply {exp-name}" to reproduce prev exp
17> Change params, re-run code (produce new experiments)
18> Now git add, commit, push

Adding a remote S3 storage to S3:
19> Login to AWS console and create an IAM user (if not already created)
    Note: the credentials for IAM user will be utilized for connecting the application to s3 bucket or any other aws services
20> Create S3 bucket (create unique bucket name)
21> install python libraries dvc[s3] and awscli if not already installed
    a> dvc[s3] : helps to connect dvc to s3 bucket
    b> awscli : this will help to configure IAM user credentials to the project application
22> Do aws configure -> on terminal
23> Do dvc remote add -d dvcstore <you can give any other name instead of dvcstore> s3://<s3 bucketname>
    This will update the config file under .dvc folder in the application with details of specified s3 bucket, thereby establishing connection between dvc and aws 
24> dvc commit and dvc push the dvc exp outcome or version that you want to keep
    the desired experiment can be reproduced using "dvc exp apply {exp-name}" as mentioned earlier
25> Finally do git add, commit and push to save the changes
26> Last but NOT least, delete the s3 bucket and IAM user details so that you dont get charged